{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL in NLP\n",
    "\n",
    "## Task 2: Classifying TED talks\n",
    "\n",
    "Sergei Volodin, senior undergraduate student at MIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from __future__ import print_function\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pylab\n",
    "from transliterate import translit\n",
    "from six.moves import range\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_unicode(ent):\n",
    "    print(repr(ent).decode(\"unicode-escape\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'ted_ru-20160408.json'\n",
    "data_test = json.loads(open(filename + '.test', 'r').read(), 'unicode-escape')\n",
    "data_train = json.loads(open(filename + '.train', 'r').read(), 'unicode-escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    "\n",
    "1. Removing non-letters\n",
    "2. Obtaining words, training word2vec CBOW model\n",
    "3. Running RNN on document\n",
    "4. Classifying based on final output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 100000\n",
    "embedding_size = 256\n",
    "context_size = 4\n",
    "words_regex = re.compile(ur'[^а-яА-ЯёЁa-zA-Z]')\n",
    "\n",
    "def str_to_words(s):\n",
    "    return(words_regex.sub(' ', s).lower().split())\n",
    "\n",
    "def data_to_str(data):\n",
    "    return(' '.join(map(lambda x : x['content'], data_train)))\n",
    "\n",
    "def build_dataset(words, vocabulary_size):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    return data, count, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_text = data_to_str(data_train)# + data_to_str(data_test)\n",
    "words = str_to_words(all_text)\n",
    "data, count, dictionary = build_dataset(words, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['UNK', 50679], (u'и', 92090), (u'в', 83510), (u'что', 57304), (u'я', 47139)]\n",
      "[4, 49, 42333, 12866, 7686, 21, 4461, 740, 105, 31181]\n"
     ]
    }
   ],
   "source": [
    "print_unicode(count[:5])\n",
    "print_unicode(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining CBOW word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def to_range(n):\n",
    "    return(n % len(data))\n",
    "\n",
    "# CBOW model\n",
    "def generate_batch(batch_size, context_size):\n",
    "    global data_index\n",
    "  \n",
    "    data_index = to_range(data_index)\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size, context_size * 2), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        labels[i] = data[data_index]\n",
    "        for j in range(0, context_size):\n",
    "            batch[i][2 * j] = data[to_range(data_index - (j + 1))]\n",
    "            batch[i][2 * j + 1] = data[to_range(data_index + (j + 1))]\n",
    "        data_index = to_range(data_index + 1)\n",
    "        \n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_sampled = 256\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size, context_size * 2], name = \"train_dataset\")\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1], name = \"train_labels\")\n",
    "  \n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), name = \"embeddings\")\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)), name = \"SM_weights\")\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]), name = \"SM_biases\")\n",
    "    \n",
    "    embed = tf.reduce_mean(tf.nn.embedding_lookup(embeddings, train_dataset), [1], name = \"emb_result\")\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size), name = \"loss\")\n",
    "\n",
    "  \n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "    \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    tf.summary.scalar(\"Embedding_loss\", loss)\n",
    "    summary_emb = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logs_path = '~/tensorboard/05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [10:40<00:00, 46.87it/s]\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.FileWriter(logs_path, graph=graph)\n",
    "epochs = 30000\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        batch_data, batch_labels = generate_batch(batch_size, context_size)\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l, summary = session.run([optimizer, loss, summary_emb], feed_dict = feed_dict)\n",
    "        writer.add_summary(summary, epoch)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_fn = \"emb.pkl\"\n",
    "#pickle.dump(final_embeddings, open(emb_fn, 'w'))\n",
    "final_embeddings = pickle.load(open(emb_fn, 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pad_word = vocabulary_size\n",
    "max_words = 5600\n",
    "num_classes = 8\n",
    "num_steps = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_to_idx(s):\n",
    "    words = str_to_words(s)\n",
    "    res = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "        res.append(index)\n",
    "    return(res)\n",
    "def get_Y(data):\n",
    "    classes = map(lambda x: x['class'] - 1, data_train)\n",
    "    return(np.array(classes))\n",
    "def get_X(data):\n",
    "    X = map(lambda x : str_to_idx(x['content']), data)\n",
    "    L = map(len, X)\n",
    "    return np.array(X), np.array(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, L = get_X(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = get_Y(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtest, Ltest = get_X(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del dictionary, data, count, all_text, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word length distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fa4abfbbfd0>]], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa479ebf1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(L).hist()\n",
    "plt.title('Train set length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fa4c37e3bd0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEh5JREFUeJzt3X+s3Xddx/Hni22wsYv7weBSu8kFnJiGKrArogy8dYDA\ngE0lY4QfhUAqiSBIUYooxsRoIQ4BISHLwHRhUuaAbDIEx+CGGDKgZcNuK2OAxVG7FtgvChMovP3j\nfDsPpe05vfeenns+Ph/Jzf3++ny/73d67+t+z+f8aKoKSdLku9+4C5AkLQ0DXZIaYaBLUiMMdElq\nhIEuSY0w0CWpEQa6NIQk1yV50Riue3ySSnL60b62Jo+BriWXZG/f10+S3Nu3/sJFnHckoZrklUk+\nudTnXYhx/eFQG44ddwFqT1VN7V9OsgN4RVUti8CUWuYduo66JMck+YskX0/y7SSXJTm523diks1J\n7khyV5LPJTklyUXArwGXdHf6Fx3kvAcd2+07NcmlSW5PcluSv0xyvySPA94OzHXnvX3IHv4gyS3d\nta5OsrLbvn+KZF2SryW5M8nf9407Nsk7k3yn2/9HSfZ1+w7X47MOdj6pn4GucXg98HTgbOB04EfA\n/pB6Bb1HjiuB04BXAT+sqvXAF+jd7U916wc66Nhu32XA3cAjgScA5wMvrqrrgdcC8915Hzao+CTP\n78Y8B5gGrgfef8BhzwAeBzweeFmSuW77q4DfAh7T1fG8/QMG9Hio80n3MdA1Dq8ENlTVf1fV/wB/\nBTw/SeiF+0OAR1XVvqr6QlV9b8jzHnRskocDTwFeV1Xfr6pdwDuBCxdR/19X1Veq6kdd/Wcnme47\n5m+q6p6q+k/gM8Bju+0XAG+rql1V9R3grUNe81Dnk+7jHLqOqi60zwA+lqT/k+HuBzwYeC/wMOCK\nJFPApcBfVNWPhzj9QccCDweOB77Vu/x91/vqAtt4OPCeJO/u27aP3qONu7v1/qmb7wP7n1f4eeC2\nvn39y4dzqPNJ9zHQdVRVVSXZCfxeVW09xGFvBt6c5JHAJ4Cb6E2ZHPajQavqB4cY+1lgL3BKHfzj\nRY/0I0dvA/6kqj504I4kxw8Yu4te8O93xiJrke7jlIvG4T3AxiRnACR5aJLndMtPTbIqyf2Ae+jd\n+f6kG7eb3hz4QR1qbDdNcR3w1iQP6p4MPTPJ2X3nPSPJcUdQ/58neXR33VOS/P6QYy8H/jjJw5I8\nmN7zCf0O26N0OAa6xuGtwCeBTyX5Lr076Md3+1YCVwLfBW4EPgZ8sNv398BLuld6HGzu+XBjXwCc\nDHwZuKPbvn/O++PADmBPkm8OKr6qPgC8C/hwknuAG4CnDdN4N+6zwM3A54GPAj/o2z+oR+mQ4n9w\nIY1Pkt8FNlbVo8ddiyafd+jSUdRN+Ty9ey3+LwB/Dnxk3HWpDd6hS0dRkpOATwO/RO+J2n8B/riq\n9o61MDXBQJekRjjlIkmNOKqvQz/ttNNqZmZmQWO/973vceKJJy5tQctEy71B2/3Z2+SapP62bt36\n7ap6yKDjjmqgz8zMsGXLlgWNnZ+fZ25ubmkLWiZa7g3a7s/eJtck9ZfkG8Mc55SLJDXCQJekRhjo\nktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wv+CTsvKzIarx3LdHRvPHct1paXkHbokNcJA\nl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEb6xaBnzTTaSjoR36JLUCO/Q9TOW+pHB+tX7\neOmYHm1I/594hy5JjTDQJakRBrokNWKoQE/yxiQ3J7kxyQeSHJ/k1CTXJLm1+37KqIuVJB3awCdF\nk8wA64BVVXVvksuBC4FVwLVVtTHJBmAD8IYR1jo2o375oE8aSloKw9yh3wP8CDghybHAA4H/Bs4D\nNnXHbALOH0mFkqShDAz0qroD+Dvgv4BdwN1V9W/AdFXt6g67HZgeWZWSpIFSVYc/IHkU8FHgKcBd\nwD8DVwDvqqqT+467s6p+Zh49yTp6UzZMT0+ftXnz5gUVunfvXqamphY0drG27bx7pOefPgF23zvS\nS4zVJPS3euVJCxo3zp/LUWu5N5is/tasWbO1qmYHHTfMG4tmgc9W1bcAknwY+E1gd5IVVbUryQpg\nz8EGV9XFwMUAs7OzNTc3N2QLP21+fp6Fjl2sUc9vr1+9j4u2tfser0nob8cL5xY0bpw/l6PWcm/Q\nZn/DzKHfAjwxyQOTBDgH2A5cBaztjlkLXDmaEiVJwxh421RVNyS5FNgC/AS4nt4d9xRweZKXA98A\nLhhloZKkwxvqcXBVvQV4ywGbf0Dvbl2StAz4TlFJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANd\nkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWp\nEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhph\noEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1IihAj3JyUmuSPLlJNuT/EaSU5Nck+TW7vsp\noy5WknRow96hvwP4eFX9MvCrwHZgA3BtVZ0JXNutS5LGZGCgJzkJeArwXoCq+mFV3QWcB2zqDtsE\nnD+qIiVJgw1zh/4I4FvAPya5PsklSU4EpqtqV3fM7cD0qIqUJA2Wqjr8AckscB3wpKr6XJJ3APcA\nr66qk/uOu7OqfmYePck6YB3A9PT0WZs3b15QoXv37mVqampBYxdr2867R3r+6RNg970jvcRYTUJ/\nq1eetKBx4/y5HLWWe4PJ6m/NmjVbq2p20HHDBPrDgOuqaqZbfzK9+fJfBOaqaleSFcB8VT36cOea\nnZ2tLVu2DNnCT5ufn2dubm5BYxdrZsPVIz3/+tX7uGjbsSO9xjhNQn87Np67oHHj/LkctZZ7g8nq\nL8lQgT5wyqWqbgduS7I/rM8BbgauAtZ229YCVy6wVknSEhj2tunVwGVJ7g98HXgZvT8Glyd5OfAN\n4ILRlChJGsZQgV5VNwAHu90/Z2nLkSQtlO8UlaRGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWp\nEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhph\noEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6\nJDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqRHHDntgkmOALcDOqnp2klOBDwIzwA7g\ngqq6cxRFSqM2s+HqBY1bv3ofL13g2P12bDx3UeOl/Y7kDv01wPa+9Q3AtVV1JnBtty5JGpOhAj3J\n6cC5wCV9m88DNnXLm4Dzl7Y0SdKRSFUNPii5Avhb4EHA67spl7uq6uRuf4A7968fMHYdsA5genr6\nrM2bNy+o0L179zI1NbWgsYu1befdIz3/9Amw+96RXmKsWu5vKXpbvfKkpSlmiY3zd+5omKT+1qxZ\ns7WqZgcdN3AOPcmzgT1VtTXJ3MGOqapKctC/DFV1MXAxwOzsbM3NHfQUA83Pz7PQsYu12DnSQdav\n3sdF24Z+OmPitNzfUvS244VzS1PMEhvn79zR0GJ/w/wkPgl4bpJnAccDP5fk/cDuJCuqaleSFcCe\nURYqSTq8gXPoVfXGqjq9qmaAC4FPVdWLgKuAtd1ha4ErR1alJGmgxbwOfSPwtCS3Ak/t1iVJY3JE\nk39VNQ/Md8vfAc5Z+pIkSQvhO0UlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQ\nJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpERPzX7Fv23k3L91w9bjLkKRl\nyzt0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXC\nQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1YmCgJzkjyaeT3JzkpiSv6bafmuSa\nJLd2308ZfbmSpEMZ5g59H7C+qlYBTwT+MMkqYANwbVWdCVzbrUuSxmRgoFfVrqr6Yrf8XWA7sBI4\nD9jUHbYJOH9URUqSBktVDX9wMgN8BngM8F9VdXK3PcCd+9cPGLMOWAcwPT191ubNmxdU6J477mb3\nvQsauuxNn0CzvUHb/S1Fb6tXnrQ0xSyxvXv3MjU1Ne4yRmaS+luzZs3WqpoddNyxw54wyRTwIeC1\nVXVPL8N7qqqSHPQvQ1VdDFwMMDs7W3Nzc8Ne8qf8w2VXctG2ocudKOtX72u2N2i7v6XobccL55am\nmCU2Pz/PQn9fJ0GL/Q31Kpckx9EL88uq6sPd5t1JVnT7VwB7RlOiJGkYw7zKJcB7ge1V9ba+XVcB\na7vltcCVS1+eJGlYwzxWfBLwYmBbkhu6bX8GbAQuT/Jy4BvABaMpUZI0jIGBXlX/DuQQu89Z2nIk\nSQvlO0UlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS\n1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmN\nMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1Ihj\nx12A9P/dzIarx3LdHRvPHct1NTreoUtSIxZ1h57kGcA7gGOAS6pq45JUJUkj0P9oaP3qfbz0KD46\nOhqPiBZ8h57kGODdwDOBVcALkqxaqsIkSUdmMVMuTwC+WlVfr6ofApuB85amLEnSkUpVLWxg8jzg\nGVX1im79xcCvV9WrDjhuHbCuW300cMsCaz0N+PYCxy53LfcGbfdnb5Nrkvp7eFU9ZNBBI3+VS1Vd\nDFy82PMk2VJVs0tQ0rLTcm/Qdn/2Nrla7G8xUy47gTP61k/vtkmSxmAxgf4F4Mwkj0hyf+BC4Kql\nKUuSdKQWPOVSVfuSvAr4BL2XLb6vqm5assp+1qKnbZaxlnuDtvuzt8nVXH8LflJUkrS8+E5RSWqE\ngS5JjVj2gZ7kGUluSfLVJBvGXc+wkrwvyZ4kN/ZtOzXJNUlu7b6f0rfvjV2PtyT5nb7tZyXZ1u17\nZ5Ic7V4OlOSMJJ9OcnOSm5K8pts+8f0lOT7J55N8Kcn2JBu77RPf235JjklyfZKPdust9bajq+uG\nJFu6bc30N1BVLdsvek+2fg14JHB/4EvAqnHXNWTtTwEeD9zYt+2twIZueQPwlm55VdfbA4BHdD0f\n0+37PPBEIMC/As9cBr2tAB7fLT8I+ErXw8T319Ux1S0fB3wOeHILvfX1+Drgn4CPtvRz2dW1Azjt\ngG3N9Dfoa7nfoU/sxwtU1WeAOw7YfB6wqVveBJzft31zVf2gqv4T+CrwhCQrgJ+rquuq91N2ad+Y\nsamqXVX1xW75u8B2YCUN9Fc9e7vV4+jdVNxJA70BJDkdOBe4pG9zE70dRuv93We5B/pK4La+9W92\n2ybVdFXt6pZvB6a75UP1ubJbPnD7spFkBngcvTvZJvrrpiRuAPYA81V1I430Brwd+FPgJ33bWukN\noIBPJtnafewItNXfYfkfXIxJVVWSiX7NaJIp4EPAa6vqnv5pxknur6p+DDw2ycnAJ5KsOWD/RPaW\n5NnAnqrammTuYMdMam99zq6qnUkeClyT5Mv9Oxvo77CW+x16ax8vsLt7OEf3fU+3/VB97uyWD9w+\ndkmOoxfml1XVh7vNzfQHUFV3AVcDs7TR25OA5ybZQW/68reTvJ82egOgqnZ23/cAH6E3bdtMf4Ms\n90Bv7eMFrgLWdstrgSv7tl+Y5AFJHgGcCXy+e5h4T5Inds+yv6RvzNh0tbwX2F5Vb+vbNfH9JXlI\nd2dOkhOApwE30EBvVfXGqjq9qmbo/S59qqpeRAO9ASQ5McmD9i8DTwdupJH+hjLuZ2UHfQHPovcq\niq8Bbxp3PUdQ9weAXcCP6M3BvRx4MHAtcCvwSeDUvuPf1PV4C33PqNO7O7yx2/cuunf3jrm3s+nN\nVf4HvbC7oft3mvj+gF8Brqf36odtwBu67RPf2wF9zvF/r3Jpojd6r4b7Uvd10/68aKW/Yb58678k\nNWK5T7lIkoZkoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RG/C+JLYBHe+AOuAAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa479515750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(Ltest).hist()\n",
    "plt.title('Test set length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#c_sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logs_path = '~/tensorboard/cl/12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lstm_size = 128\n",
    "c_graph = tf.Graph()\n",
    "with c_graph.as_default():\n",
    "    text_input = tf.placeholder(tf.int32, shape=[None, None])\n",
    "    text_length = tf.placeholder(tf.float32, shape=[None])\n",
    "    ans_input = tf.placeholder(tf.int32, shape=[None])\n",
    "    do_update = tf.placeholder(tf.float32, shape=[None])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    cell_state = tf.placeholder(tf.float32, [batch_size, lstm_size])\n",
    "    hidden_state = tf.placeholder(tf.float32, [batch_size, lstm_size])\n",
    "    initial_state = tf.contrib.rnn.LSTMStateTuple(cell_state, hidden_state)\n",
    "    \n",
    "    labels = tf.one_hot(ans_input, num_classes)\n",
    "    \n",
    "    emb_words   = tf.Variable(final_embeddings, name = \"embeddings\", trainable = True, dtype = tf.float32)\n",
    "    emb_padding = tf.Variable(1. * np.zeros((1, embedding_size)), trainable = False, dtype = tf.float32)\n",
    "    embeddings = tf.concat([emb_words, emb_padding], 0)\n",
    "    \n",
    "    word_vectors = tf.reshape(tf.nn.embedding_lookup(embeddings, text_input), [-1, num_steps, embedding_size])\n",
    "    word_list = tf.unstack(tf.nn.dropout(word_vectors, keep_prob), axis = 1)\n",
    "    \n",
    "    # LSTM\n",
    "    cell = tf.contrib.rnn.LSTMCell(num_units = lstm_size)\n",
    "    _, current_state = tf.contrib.rnn.static_rnn(cell, word_list, initial_state = initial_state,\n",
    "                                               sequence_length = text_length)\n",
    "    out = current_state[1]\n",
    "    # /LSTM\n",
    "    \n",
    "    num_updates = tf.reduce_sum(do_update)\n",
    "    \n",
    "    dense = tf.nn.dropout(tf.layers.dense(inputs = out, units=64, activation=tf.nn.relu), keep_prob)\n",
    "    logits = tf.contrib.layers.fully_connected(dense, num_classes, activation_fn = None)\n",
    "    \n",
    "    sm_loss = tf.nn.softmax_cross_entropy_with_logits(labels = labels, logits = logits)\n",
    "    loss = tf.reduce_mean(tf.transpose((tf.transpose(sm_loss) * do_update))) / (num_updates + 0.001) * batch_size\n",
    "    \n",
    "    optimizer = tf.train.RMSPropOptimizer(0.01).minimize(loss)\n",
    "    \n",
    "    tf.summary.scalar(\"Classification_loss\", loss)\n",
    "    summary = tf.summary.merge_all()\n",
    "    v_summary = tf.summary.scalar(\"valid_loss\", loss)\n",
    "    \n",
    "    answer = tf.argmax(logits, 1)\n",
    "    probability = tf.nn.softmax(logits)\n",
    "    \n",
    "    initializer = tf.global_variables_initializer()\n",
    "    writer = tf.summary.FileWriter(logs_path, c_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(X, Y, L, size = batch_size):\n",
    "    N = len(X)\n",
    "    start_idx = 0\n",
    "    while start_idx < N:\n",
    "        do_clear_state = 1\n",
    "        idx = range(start_idx, start_idx + size)\n",
    "        if np.max(idx) >= N:\n",
    "            break\n",
    "        max_len = np.max(L[idx])\n",
    "        start_pos = 0\n",
    "        while start_pos < max_len:\n",
    "            X_ = []\n",
    "            L_ = []\n",
    "            Last = []\n",
    "            for i in idx:\n",
    "                v = X[i][start_pos : min(start_pos + num_steps, L[i])]\n",
    "                if start_pos + num_steps >= L[i] and start_pos < L[i]:\n",
    "                    Last.append(1)\n",
    "                else:\n",
    "                    Last.append(0)\n",
    "                L_.append(len(v))\n",
    "                v += [pad_word] * (num_steps - len(v))\n",
    "                X_.append(v)\n",
    "            yield np.array(X_), Y[idx], np.array(L_), do_clear_state, np.array(Last)\n",
    "            do_clear_state = 0\n",
    "            start_pos += num_steps\n",
    "        start_idx += size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_sess = tf.Session(graph=c_graph)\n",
    "c_sess.run(initializer)\n",
    "rolling_epoch = 0\n",
    "_current_cell_state = np.zeros((batch_size, lstm_size))\n",
    "_current_hidden_state = np.zeros((batch_size, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1 0.0 1\n",
      "0.0 2 0.0 0\n",
      "0.0 3 0.0 0\n",
      "1.0 4 2.08592 0\n",
      "0.0 5 0.0 0\n",
      "0.0 6 0.0 0\n",
      "1.0 7 2.07634 0\n",
      "1.0 8 2.05902 0\n",
      "1.0 9 2.0452 0\n",
      "1.0 10 2.12533 0\n",
      "2.0 11 2.04053 0\n",
      "0.0 12 0.0 0\n",
      "0.0 13 0.0 0\n",
      "0.0 14 0.0 0\n",
      "3.0 15 2.06762 0\n",
      "1.0 16 2.04921 0\n",
      "3.0 17 2.05207 0\n",
      "1.0 18 1.99307 0\n",
      "3.0 19 2.04567 0\n",
      "3.0 20 2.01968 0\n",
      "1.0 21 1.86182 0\n",
      "3.0 22 2.08992 0\n",
      "1.0 23 2.09329 0\n",
      "1.0 24 2.16966 0\n",
      "1.0 25 1.95685 0\n",
      "2.0 26 1.70712 0\n",
      "2.0 27 1.61739 0\n",
      "1.0 28 1.75379 0\n",
      "2.0 29 1.83467 0\n",
      "2.0 30 2.29189 0\n",
      "2.0 31 1.67673 0\n",
      "1.0 32 1.71337 0\n",
      "0.0 33 0.0 0\n",
      "1.0 34 2.34361 0\n",
      "1.0 35 1.44821 0\n",
      "0.0 36 0.0 0\n",
      "0.0 37 0.0 0\n",
      "0.0 38 0.0 0\n",
      "0.0 39 0.0 0\n",
      "2.0 40 1.70888 0\n",
      "1.0 41 2.57004 0\n",
      "0.0 42 0.0 0\n",
      "1.0 43 1.91807 0\n",
      "0.0 44 0.0 0\n",
      "0.0 45 0.0 0\n",
      "1.0 46 1.52675 0\n",
      "2.0 47 1.3873 0\n",
      "0.0 48 0.0 0\n",
      "3.0 49 2.0818 0\n",
      "3.0 50 1.39987 0\n",
      "1.0 51 1.45636 0\n",
      "1.0 52 1.042 0\n",
      "2.0 53 3.28305 0\n",
      "1.0 54 1.09535 0\n",
      "1.0 55 0.995346 0\n",
      "2.0 56 0.853368 0\n",
      "3.0 57 1.48504 0\n",
      "1.0 58 0.829235 0\n",
      "1.0 59 3.52561 0\n",
      "2.0 60 0.681071 0\n",
      "3.0 61 0.696298 0\n",
      "4.0 62 0.221939 0\n",
      "0.0 63 0.0 0\n",
      "0.0 64 0.0 0\n",
      "3.0 65 0.147942 0\n",
      "1.0 66 3.08722 0\n",
      "2.0 67 1.62137 0\n",
      "3.0 68 2.46301 0\n",
      "1.0 69 1.70162 0\n",
      "2.0 70 1.40502 0\n",
      "0.0 71 0.0 0\n",
      "0.0 72 0.0 0\n",
      "0.0 73 0.0 0\n",
      "4.0 74 1.52501 0\n",
      "3.0 75 1.02365 0\n",
      "1.0 76 2.32709 0\n",
      "0.0 77 0.0 0\n",
      "3.0 78 1.23608 0\n",
      "2.0 79 1.03081 0\n",
      "2.0 80 1.2882 0\n",
      "2.0 81 3.52534 0\n",
      "2.0 82 1.42361 0\n",
      "0.0 83 0.0 0\n",
      "1.0 84 2.85644 0\n",
      "1.0 85 1.34238 0\n",
      "1.0 86 1.31317 0\n"
     ]
    }
   ],
   "source": [
    "valid_idx = random.sample(range(len(X)), 100)\n",
    "train_idx = np.setdiff1d(np.arange(X.shape[0]), valid_idx)\n",
    "\n",
    "while True:\n",
    "    for X_, Y_, L_, do_clear_state, Last in get_minibatch(X[train_idx], Y[train_idx], L[train_idx], batch_size):\n",
    "        feed_dict = {text_input: X_, ans_input: Y_, text_length: L_,\n",
    "                     cell_state: _current_cell_state, hidden_state: _current_hidden_state, keep_prob: 0.5,\n",
    "                    #do_update: 1. * (L_ > 0)}\n",
    "                     do_update: Last}\n",
    "        \n",
    "        n, _, l, s, c_state = c_sess.run([num_updates, optimizer, loss, summary, current_state], feed_dict = feed_dict)\n",
    "        _current_cell_state, _current_hidden_state = c_state\n",
    "        if n > 0:\n",
    "            writer.add_summary(s, rolling_epoch)\n",
    "        \n",
    "        rolling_epoch += 1\n",
    "        print(n, rolling_epoch, l, do_clear_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_idx(X, L, idx):\n",
    "    start_pos = 0\n",
    "    max_len = min(np.max(L[idx]), 100)\n",
    "    _current_cell_state = np.zeros((batch_size, lstm_size))\n",
    "    _current_hidden_state = np.zeros((batch_size, lstm_size))\n",
    "    res = np.zeros((batch_size, num_classes))\n",
    "    while start_pos < max_len:\n",
    "        #print(start_pos)\n",
    "        X_ = []\n",
    "        L_ = []\n",
    "        \n",
    "        for i in idx:\n",
    "            v = X[i][start_pos : min(start_pos + num_steps, L[i])]\n",
    "            L_.append(len(v))\n",
    "            v += [pad_word] * (num_steps - len(v))\n",
    "            X_.append(v)\n",
    "    \n",
    "        feed_dict = {text_input: X_, text_length: L_, cell_state: _current_cell_state,\n",
    "                     hidden_state: _current_hidden_state, keep_prob: 1}\n",
    "        \n",
    "        ll, c_state = c_sess.run([probability, current_state], feed_dict = feed_dict)\n",
    "        _current_cell_state, _current_hidden_state = c_state\n",
    "        \n",
    "        print(ll)\n",
    "        \n",
    "        res += ll\n",
    "        \n",
    "        start_pos += num_steps\n",
    "    #print(ll)\n",
    "    answers = np.argmax(ll, axis = 1)\n",
    "    return(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classify_idx(X, L, range(1000,1000+64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.InteractiveSession at 0x7f6027ea8f10>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.Variable(np.array([[1.,2,3],[4,5,6],[9, 8, 7]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.Variable(np.array([0.,1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.initializer.run()\n",
    "y.initializer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.],\n",
       "       [  4.,   5.,   6.],\n",
       "       [ 18.,  16.,  14.]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose((tf.transpose(X) * y)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
