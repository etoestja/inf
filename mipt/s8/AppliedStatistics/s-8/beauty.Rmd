---
title: "Привлекательность и уровень заработной платы"
output: html_document
---
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(lattice)
library(MASS)
library(lmtest)
library(sandwich)
library(mvtnorm)
library(car)
library(multcomp)

mycol <- rgb(30,30,30,100,maxColorValue=255)

mycoeftest <- function(m, EstType){
  beta  <- coef(m)[-1]
  Vbeta <- vcovHC(m, type = EstType)[-1,-1]
  D <- diag(1 / sqrt(diag(Vbeta)))
  t <- D %*% beta
  Cor <- D %*% Vbeta %*% t(D)
  m.df <- length(m$residuals) - length(beta)
  p_adj <- sapply(abs(t), function(x) 1-pmvt(-rep(x, length(beta)), rep(x, length(beta)), 
                                             corr = Cor, df = m.df))
  c(NaN, p_adj)
}

addtrend <- function(x, y){
  y <- y[order(x)]
  x <- sort(x)  
  lines(x, predict(loess(y ~ x)), col = "red")
}
```

## Постановка задачи
По 1260 опрошенным имеются следующие данные:

* заработная плата за час работы, $;
* опыт работы, лет;
* образование, лет;
* внешняя привлекательность, в баллах от 1 до 5;
* бинарные признаки: пол, семейное положение, состояние здоровья (хорошее/плохое), членство в профсоюзе, цвет кожи (белый/чёрный), занятость в сфере обслуживания (да/нет).

Требуется оценить влияние внешней привлекательности на уровень заработка с учётом всех остальных факторов.

Попарные диаграммы рассеяния всех количественных признаков:
```{r, echo=FALSE, fig.height=10, fig.width=10}
data <- read.csv("beauty.csv", sep=";")

panel.hist <- function(x, ...){
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "red", ...)
}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...){
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

panel.dots <- function(x, y, ...){
  points(x, y, pch=19, col=mycol)
}

pairs(data[,c("wage", "exper", "educ", "looks")], diag.panel=panel.hist, 
      upper.panel = panel.cor, lower.panel = panel.dots)
```

Можно более коротко: 
```{r}
library(car)
scatterplotMatrix(data[,c("wage", "exper", "educ", "looks")], spread=FALSE, smooth=T,
    main="Scatter Plot Matrix")
```


## Задание
### Предобработка
1. Посмотрите на распределение оценок привлекательности (`looks`). Сделайте из него категориальный признак, разбив на три категории (низкая привлекательность (`looks < 3`), средняя привлекательность (`looks = 3`) и высокая привлекательность (`looks > 3`))
```{r}
# type your code here
```


2. Посмотрите на распределение значений отклика. Есть ли в нем выбросы? Выбросы могут сильно влиять на   коэффициенты регрессии и мы с вами еще  рассмотрим способы выявления выбросов.  Удалите наблюдения, которые вы считаете выбросами.  Этот шаг нужно всегда делать осторожно, но в этих данных выбросы видны невооруженным взглядом. 

```{r}
# Type your code here
```



3. Видно, что распределение откликов далеко от нормального, поэтому примените пробразование Бокса-Кокса с помощью функции `boxcox`  из пакета `MASS`. На вход этой функции нужно передать результат выполнения вызова построения линейной регресии `lm`, где в виде формулы нужно будет закодировать зависимость отклика от всех возможных объясняющих переменных.  Преобразуйте отклик согласно той трансформации, которую вы получите в результате выполнения этой функции.  

```{r}
# Type your code here
```


### Модель 1

4. Постройте линейную регрессию по всем признакам, используя преобразованные значения отклика. Посмотрите на остатки регрессии c помощью функции `residuals(lm(...))` 

* Проверьте остатки на нормальность с помощью критерия Шапиро-Уилка.   

    * Если гипотеза нормальности отвергается, то для проверки несмещенности воспользуейтесь критерием знаковых рангов Уилкоксона (`wilcox.test`), не забыв при этом отключить коррекцию на непрерывность. 

    * Если гипотеза нормальности не отвергается, то воспользуйтесь критерием Стьюдента. 

* С помощью критерия Бройша-Пагана (`bptest` из `lmtest`) проверьте модель на наличие гетероскедастичности. 



```{r}
# Type your code here 
```

 
5. Если гипотеза об отсутсвии гетероскедастичности отвергается, то рекомендуется воспользоваться специальным подходом отбора признаков, который был на лекции и который реализует функция `mycoeftest`. Вторым аргументом в эту функцию нужно передать строку `"HC0"`, которая соответствует устойчивой оценке дисперсии Уайта. Получите с помощью функции `mycoeftest` значения достигаемых уровней значимости,  скорректированных на множественную проверку гипотез. 


```{r}
# Type your code here 
```


6. Проведите визуальный анализ остатков. 

* проведите визуальный анализ нормальности остатков с помощью функции `qqnorm / qqline`

* нарисуйте график зависимости нормализованных значений остатков `rstudent(lm(...))` от каждого из признаков и добавьте тренд к каждому из графиков с помощью функции `addtrend`. Видите ли вы какие-то зависимости, которые можно было бы добавить к исходному набору признаков? 


```{r}
# Type your code here 
```


### Модель 2

7. Добавьте в линейную модель новый признак - квадрат опыта работы, положив в вызове функции `lm` формулу равной  `logwage ~ . + I(exper^2)`. Здесь функция `I()` экранирует вычисление поэлементного возведения в степень от логики обработки формул в R. Если не написать внешнее `I()`, то в модели окажется не квадрат `exper`, а взаимодействие между всеми элементами, которые возводятся в степень. То есть, поскольку у нас возводится только один элемент в степень, то и никаких дополнительных взаимодействий в модель добавлено не будет.
```{r}
# Type your code here 
```

8. Убедитесь аналогично тому, как вы это делали ранее, что остатки этой регрессии ненормальны и гетерскедастичны. А также осуществите отбор признаков с учетом множестенной проверки гипотез (так же, как в предыдущем пункте). Какие признаки оказались незначимы? 

```{r}
# Type your code here 
```


9.  Проведите визуальный анализ нормальности остатков и зависимостей остатков от каждого из признаков в модели, которую вы построили. Можно ли еще добавить какие-либо преобразования признаков, которые улучшат эти зависимости? 

```{r}
# Type your code here 
```

10. Прежде чем удалять незначимые признаки, следует проверить, что что эти признаки не входят в значимые взаимодействия с другими признаками. Для этого воспользуйстесь функцией `add1`, которой передайте формулу `~ .^2` (что означает взять все возможные взаимодействия в модели, которую вы передадите первым аргументом) и укажите аргумент `test="F"`

```{r}
# Type your code here 
```


### Модель 3

11. Удалите из из модели 2 незначимые признаки и добавьте взаимодействие пола и опыта работы,  а также добавьте в модель `aboveavg`. Проверьте остатки на нормальность, несмещенность и гетероскедастичность. Проверьте коэффициенты на значимость с учетом коррекции на множественное сравнение. 

```{r}
# Type your code here 
```

12. Сравните модель 2 с моделью 3 при помощью критерия Давидсона-Маккинона, который реализует функция `jtest` из пакета `lmtest`. Какая из двух моделей лучше ? 

```{r}
# Type your code here 
```



### Модель 4

13. Попробуем оставить в модели 2 цвет кожи и семейное положение, чтобы добавить их взаимодействия с полом. Как и в модели 3, добавьте взаимодействие пола с опытом работы, удалите состояние здоровья и добавьте  `aboveavg`. 


```{r}
# Type your code here 
```


14.   Проверьте остатки на нормальность, несмещенность и гетероскедастичность. Проверьте коэффициенты на значимость с учетом коррекции на множественное сравнение.


```{r}
# Type your code here 
```


15. Проведите визуальный анализ остатков для построенной Вами модели. 
```{r}
# Type your code here 
```


16. Сравните построенную модель с моделью 3 c помощью критерия Вальда с дисперсиями Уайта (в случае гетероскедастичности). В этом Вам поможет функция `waldtest` из пакета `lmtest`. Обатите внимание на параметр `vcov=vcovHC(lm(..), type="HC0")`. Получается ли значимо лучше, чем модель 3? 
```{r}
# Type your code here 
```


17. Посмотрите, есть ли еще значимые взаимодействия, которые можно добавить в модель (функция `add1`). Рассмотрите только те взаимодействия, которые значимы на уровне 0.01. Если таких не окажется, переходите к следующему пункту. Кроме того, стоит избегать сложных взаимодействий, которые затруднительно интепретировать (например, взаимодействие квадрата опыта и цвета кожи).

```{r}
# Type your code here 
```


### Модель 5

18. Есть ли в предыдущей модели такие признаки, которые не значимы сами по себе, а также незначимы их взаимодействия? Попробуйте удалить их из модели (как отдельные признаки, так и их взаимодействия). Сравните получившуюся модель с моделью 4 при помощью теста Вальда.

```{r}
# Type your code here 
```


19. Попробуйте удалить из модели только незначимые взаимодействия. Сравните получившуюся модель с моделью 4 при помощью критерия Вальда. На какой модели Вы бы предложили остановиться?

```{r}
# Type your code here 
```


# Расстояние Кука
20. Нарисуйте график, на котором по горизонтальной оси отложите предсказанные с помощью 4ой модели значения отклика для тех данных, с которыми мы работаем (можете воспользоваться функцией `fitted`), а по вертикальной оси - расстояния Кука (`cooks.distance` из пакета `stats`). Визуально выберите порог, по котрому удалите все слишком влиятельные точки. 

```{r}
# Type your code here 
```


21. Постройте модель, аналогичную модели 4, но с удаленными влиятельными наблюдениями. Изменились ли коэффициенты при различных признаках? Проверьте гипотезу о значимости коэффициентов в модели с использованием коррекции на множественное сравнение.

```{r}
# Type your code here 
```

22. Остановитесь на лучшей, на Ваш взгляд модели и проинтерпретируйте коэффициенты перед интересующими нас признаками: что можно сказать о том, как влияет внешность на зарплату? В ответе на этот вопрос воспользуйтесь функцией `confint` для постоения доверительных интервалов для коэффициентов в модели.

```{r}
# Type your code here 
```

***************
Hamermesh D.S., Biddle J.E. (1994) **Beauty and the Labor Market**, American Economic Review, 84, 1174–1194.
