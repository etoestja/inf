{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL in NLP\n",
    "\n",
    "## Task 2: Classifying TED talks\n",
    "\n",
    "Sergei Volodin, senior undergraduate student at MIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from __future__ import print_function\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pylab\n",
    "from transliterate import translit\n",
    "from six.moves import range\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_unicode(ent):\n",
    "    print(repr(ent).decode(\"unicode-escape\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'ted_ru-20160408.json'\n",
    "data_test = json.loads(open(filename + '.test', 'r').read(), 'unicode-escape')\n",
    "data_train = json.loads(open(filename + '.train', 'r').read(), 'unicode-escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    "\n",
    "1. Removing non-letters\n",
    "2. Obtaining words, training word2vec CBOW model\n",
    "3. Running RNN on document\n",
    "4. Classifying based on final output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 100000\n",
    "embedding_size = 256\n",
    "context_size = 4\n",
    "words_regex = re.compile(ur'[^а-яА-ЯёЁa-zA-Z]')\n",
    "\n",
    "def str_to_words(s):\n",
    "    return(words_regex.sub(' ', s).lower().split())\n",
    "\n",
    "def data_to_str(data):\n",
    "    return(' '.join(map(lambda x : x['content'], data_train)))\n",
    "\n",
    "def build_dataset(words, vocabulary_size):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    return data, count, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_text = data_to_str(data_train)# + data_to_str(data_test)\n",
    "words = str_to_words(all_text)\n",
    "data, count, dictionary = build_dataset(words, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['UNK', 50679], (u'и', 92090), (u'в', 83510), (u'что', 57304), (u'я', 47139)]\n",
      "[4, 49, 42333, 12866, 7686, 21, 4461, 740, 105, 31181]\n"
     ]
    }
   ],
   "source": [
    "print_unicode(count[:5])\n",
    "print_unicode(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pad_word = vocabulary_size\n",
    "num_classes = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_to_idx(s):\n",
    "    words = str_to_words(s)\n",
    "    res = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "        res.append(index)\n",
    "    return(res)\n",
    "def get_Y(data):\n",
    "    classes = map(lambda x: x['class'] - 1, data_train)\n",
    "    return(np.array(classes))\n",
    "def get_X(data):\n",
    "    X = map(lambda x : str_to_idx(x['content']), data)\n",
    "    L = map(len, X)\n",
    "    return np.array(X), np.array(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, L = get_X(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = get_Y(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtest, Ltest = get_X(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wf(X):\n",
    "    N = len(X)\n",
    "    res = np.zeros((N, vocabulary_size))\n",
    "    for i in range(N):\n",
    "        M = len(X[i])\n",
    "        for j in tqdm(range(M):\n",
    "            res[i][X[i][j]] += 1\n",
    "        res[i] /= N\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcount = get_wf(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XTestCount = get_wf(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02439024,  0.0652439 ,  0.07317073, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.00609756,  0.00365854,  0.00670732, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.0054878 ,  0.01463415,  0.00670732, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.03414634,  0.05243902,  0.03902439, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.01280488,  0.03109756,  0.04329268, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.0195122 ,  0.04634146,  0.04268293, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = '/home/sergei/tensorboard/cl/06'\n",
    "c_graph = tf.Graph()\n",
    "with c_graph.as_default():\n",
    "    inp = tf.placeholder(tf.float32, shape=[None, vocabulary_size])\n",
    "    ans = tf.placeholder(tf.int32, shape=[None])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    labels = tf.one_hot(ans, num_classes)\n",
    "    dense = tf.layers.dense(inputs = tf.nn.dropout(inp, keep_prob), units=128, activation=tf.nn.relu)\n",
    "    logits = tf.contrib.layers.fully_connected(dense, num_classes, activation_fn = None)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = labels, logits = logits))\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.05).minimize(loss)\n",
    "    \n",
    "    summary = tf.summary.scalar(\"Classification_loss\", loss)\n",
    "    v_summary = tf.summary.scalar(\"validation_loss\", loss)\n",
    "    summary = tf.summary.merge_all()\n",
    "    \n",
    "    answer = tf.argmax(logits, 1)\n",
    "    probability = tf.nn.softmax(logits)\n",
    "    \n",
    "    initializer = tf.global_variables_initializer()\n",
    "    writer = tf.summary.FileWriter(logs_path, c_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_sess = tf.Session(graph=c_graph)\n",
    "c_sess.run(initializer)\n",
    "rolling_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.06454 2.07944\n",
      "2 2.04236 2.06299\n",
      "3 2.01999 2.04115\n",
      "4 2.00176 2.02971\n",
      "5 1.97915 2.00257\n",
      "6 1.95337 1.96949\n",
      "7 1.92561 1.93242\n",
      "8 1.89747 1.89774\n",
      "9 1.87334 1.89245\n",
      "10 1.84859 1.86235\n",
      "11 1.82428 1.84147\n",
      "12 1.79696 1.78588\n",
      "13 1.7569 1.61201\n",
      "loop\n",
      "14 1.73567 1.75736\n",
      "15 1.71184 1.69661\n",
      "16 1.69077 1.69488\n",
      "17 1.6768 1.76987\n",
      "18 1.65826 1.68081\n",
      "19 1.63746 1.608\n",
      "20 1.61626 1.54434\n",
      "21 1.59775 1.51012\n",
      "22 1.58517 1.57835\n",
      "23 1.57374 1.54784\n",
      "24 1.56385 1.55066\n",
      "25 1.55471 1.46587\n",
      "26 1.54114 1.08486\n",
      "loop\n",
      "27 1.53631 1.53421\n",
      "28 1.53228 1.44545\n",
      "29 1.52963 1.49893\n",
      "30 1.52638 1.66368\n",
      "31 1.52329 1.5235\n",
      "32 1.52018 1.44012\n",
      "33 1.51784 1.36898\n",
      "34 1.51802 1.35743\n",
      "35 1.51325 1.4728\n",
      "36 1.51145 1.44248\n",
      "37 1.50879 1.46331\n",
      "38 1.51058 1.39857\n",
      "39 1.53397 0.897994\n",
      "loop\n",
      "40 1.51559 1.50346\n",
      "41 1.51438 1.39483\n",
      "42 1.51001 1.47333\n",
      "43 1.51129 1.62377\n",
      "44 1.50781 1.48356\n",
      "45 1.50504 1.41712\n",
      "46 1.50805 1.32486\n",
      "47 1.51197 1.3318\n",
      "48 1.50261 1.43468\n",
      "49 1.50616 1.40846\n",
      "50 1.50486 1.42987\n",
      "51 1.51103 1.3984\n",
      "52 1.59167 0.85988\n",
      "loop\n",
      "53 1.53129 1.55833\n",
      "54 1.51954 1.40463\n",
      "55 1.50913 1.46536\n",
      "56 1.52664 1.61205\n",
      "57 1.50868 1.47756\n",
      "58 1.5102 1.40707\n",
      "59 1.52102 1.29605\n",
      "60 1.51876 1.32983\n",
      "61 1.50896 1.41709\n",
      "62 1.51962 1.4007\n",
      "63 1.51483 1.4242\n",
      "64 1.53269 1.40414\n",
      "65 1.69777 0.84366\n",
      "loop\n",
      "66 1.62785 1.64871\n",
      "67 1.51658 1.50561\n",
      "68 1.50746 1.4523\n",
      "69 1.53148 1.60869\n",
      "70 1.51138 1.48256\n",
      "71 1.51372 1.40683\n",
      "72 1.52759 1.29131\n",
      "73 1.52159 1.33129\n",
      "74 1.51452 1.41554\n",
      "75 1.53529 1.40218\n",
      "76 1.52027 1.43469\n",
      "77 1.54457 1.40852\n",
      "78 1.78065 0.838413\n",
      "loop\n",
      "79 1.66244 1.70678\n",
      "80 1.50573 1.54612\n",
      "81 1.50699 1.44868\n",
      "82 1.52013 1.61117\n",
      "83 1.51224 1.47758\n",
      "84 1.51291 1.40937\n",
      "85 1.52972 1.29353\n",
      "86 1.52277 1.33146\n",
      "87 1.51354 1.41533\n",
      "88 1.5411 1.40087\n",
      "89 1.52236 1.43591\n",
      "90 1.53562 1.40916\n",
      "91 1.82758 0.846725\n",
      "loop\n",
      "92 1.60134 1.7437\n",
      "93 1.5103 1.48546\n",
      "94 1.50659 1.45459\n",
      "95 1.51628 1.60329\n",
      "96 1.51294 1.47086\n",
      "97 1.51249 1.4088\n",
      "98 1.5288 1.29578\n",
      "99 1.52487 1.33239\n",
      "100 1.51399 1.41451\n",
      "101 1.53765 1.39861\n",
      "102 1.52364 1.42764\n",
      "103 1.52881 1.41008\n",
      "104 1.83145 0.847253\n",
      "loop\n",
      "105 1.57594 1.73971\n",
      "106 1.51336 1.45509\n",
      "107 1.50745 1.45506\n",
      "108 1.51471 1.59984\n",
      "109 1.51474 1.46537\n",
      "110 1.5132 1.40871\n",
      "111 1.52776 1.29546\n",
      "112 1.52704 1.33263\n",
      "113 1.51617 1.41402\n",
      "114 1.53705 1.40035\n",
      "115 1.5272 1.42078\n",
      "116 1.52495 1.41169\n",
      "117 1.79333 0.858189\n",
      "loop\n",
      "118 1.56498 1.69695\n",
      "119 1.5142 1.43571\n",
      "120 1.50877 1.45016\n",
      "121 1.51455 1.60289\n",
      "122 1.51524 1.4582\n",
      "123 1.51359 1.40852\n",
      "124 1.5269 1.29267\n",
      "125 1.52684 1.33106\n",
      "126 1.51733 1.40792\n",
      "127 1.53421 1.39324\n",
      "128 1.52615 1.41578\n",
      "129 1.52681 1.40754\n",
      "130 1.77098 0.857854\n",
      "loop\n",
      "131 1.5598 1.66745\n",
      "132 1.51589 1.42809\n",
      "133 1.50902 1.44508\n",
      "134 1.51323 1.60181\n",
      "135 1.51421 1.45188\n",
      "136 1.51295 1.40626\n",
      "137 1.52474 1.29164\n",
      "138 1.5283 1.32689\n",
      "139 1.51866 1.40907\n",
      "140 1.5333 1.391\n",
      "141 1.52891 1.40979\n",
      "142 1.52118 1.40997\n",
      "143 1.72517 0.843201\n",
      "loop\n",
      "144 1.54955 1.61956\n",
      "145 1.52188 1.4075\n",
      "146 1.50945 1.44981\n",
      "147 1.51288 1.60369\n",
      "148 1.51512 1.45341\n",
      "149 1.51253 1.40638\n",
      "150 1.52385 1.28984\n",
      "151 1.53187 1.31707\n",
      "152 1.51842 1.40336\n",
      "153 1.53559 1.38301\n",
      "154 1.52872 1.39968\n",
      "155 1.51976 1.40369\n",
      "156 1.74853 0.838454\n",
      "loop\n",
      "157 1.57302 1.62231\n",
      "158 1.51684 1.42133\n",
      "159 1.50814 1.44327\n",
      "160 1.5145 1.6035\n",
      "161 1.50876 1.45301\n",
      "162 1.51379 1.3972\n",
      "163 1.52826 1.27745\n",
      "164 1.52626 1.30896\n",
      "165 1.514 1.39861\n",
      "166 1.53584 1.37526\n",
      "167 1.52272 1.40961\n",
      "168 1.51839 1.402\n",
      "169 1.76728 0.819833\n",
      "loop\n",
      "170 1.57694 1.6463\n",
      "171 1.50999 1.40558\n",
      "172 1.50009 1.42697\n",
      "173 1.51364 1.58141\n",
      "174 1.5025 1.44614\n",
      "175 1.50249 1.3994\n",
      "176 1.51382 1.26189\n",
      "177 1.5242 1.29511\n",
      "178 1.50055 1.38798\n",
      "179 1.52091 1.3745\n",
      "180 1.5073 1.38581\n",
      "181 1.50246 1.39797\n",
      "182 1.72029 0.748137\n",
      "loop\n",
      "183 1.55381 1.58661\n",
      "184 1.4972 1.3907\n",
      "185 1.48292 1.42949\n",
      "186 1.50311 1.56746\n",
      "187 1.48612 1.42605\n",
      "188 1.48858 1.37952\n",
      "189 1.50104 1.24647\n",
      "190 1.50646 1.28514\n",
      "191 1.47599 1.37173\n",
      "192 1.50596 1.35842\n",
      "193 1.47827 1.37134\n",
      "194 1.48528 1.3538\n",
      "195 1.70611 0.649854\n",
      "loop\n",
      "196 1.58206 1.57479\n",
      "197 1.46644 1.40781\n",
      "198 1.45178 1.37565\n",
      "199 1.47391 1.52318\n",
      "200 1.45207 1.39149\n",
      "201 1.46285 1.32906\n",
      "202 1.47666 1.2288\n",
      "203 1.4759 1.25469\n",
      "204 1.43661 1.33837\n",
      "205 1.46815 1.3348\n",
      "206 1.43439 1.31944\n",
      "207 1.4472 1.32556\n",
      "208 1.62808 0.5577\n",
      "loop\n",
      "209 1.64002 1.4925\n",
      "210 1.44607 1.47952\n",
      "211 1.40058 1.34588\n",
      "212 1.43696 1.46013\n",
      "213 1.40261 1.3406\n",
      "214 1.40967 1.26796\n",
      "215 1.44132 1.15062\n",
      "216 1.40451 1.18993\n",
      "217 1.37997 1.24948\n",
      "218 1.39741 1.28796\n",
      "219 1.37609 1.28041\n",
      "220 1.40742 1.2661\n",
      "221 1.46404 0.317916\n",
      "loop\n",
      "222 1.40981 1.30297\n",
      "223 1.52255 1.23993\n",
      "224 1.42968 1.37818\n",
      "225 1.34594 1.40224\n",
      "226 1.34979 1.23667\n",
      "227 1.44803 1.23634\n",
      "228 1.3574 1.12836\n",
      "229 1.35733 1.12068\n",
      "230 1.32232 1.16599\n",
      "231 1.35526 1.19986\n",
      "232 1.43312 1.21175\n",
      "233 1.34415 1.23616\n",
      "234 1.37508 0.240073\n",
      "loop\n",
      "235 1.39927 1.21472\n",
      "236 1.54189 1.20696\n",
      "237 1.36646 1.34601\n",
      "238 1.27992 1.30686\n",
      "239 1.28243 1.13488\n",
      "240 1.33323 1.13121\n",
      "241 1.32329 1.03897\n",
      "242 1.3088 1.03706\n",
      "243 1.29073 1.08634\n",
      "244 1.41762 1.17989\n",
      "245 1.53238 1.24249\n",
      "246 1.30655 1.26986\n",
      "247 1.31398 0.212146\n",
      "loop\n",
      "248 1.28874 1.13134\n",
      "249 1.28594 1.02318\n",
      "250 1.39801 1.12335\n",
      "251 1.68009 1.40582\n",
      "252 1.3372 1.43648\n",
      "253 1.28632 1.12016\n",
      "254 1.28982 0.987075\n",
      "255 1.25837 0.953237\n",
      "256 1.24805 1.01218\n",
      "257 1.26458 1.05711\n",
      "258 1.26483 1.00748\n",
      "259 1.24852 1.02641\n",
      "260 1.28323 0.0823623\n",
      "loop\n",
      "261 1.25339 1.02226\n",
      "262 1.34155 0.954652\n",
      "263 1.27206 1.08211\n",
      "264 1.34991 1.131\n",
      "265 1.26151 1.03269\n",
      "266 1.30285 0.989934\n",
      "267 1.23659 0.882827\n",
      "268 1.22614 0.853372\n",
      "269 1.3265 0.889848\n",
      "270 1.64388 1.03692\n",
      "271 1.6207 1.3506\n",
      "272 1.23911 1.17117\n",
      "273 1.23406 0.0647434\n",
      "loop\n",
      "274 1.35833 0.991027\n",
      "275 1.28035 0.922675\n",
      "276 1.4819 1.0873\n",
      "277 1.52387 1.44678\n",
      "278 1.26344 1.11849\n",
      "279 1.20877 0.92841\n",
      "280 1.26214 0.817125\n",
      "281 1.18961 0.827137\n",
      "282 1.2573 0.844942\n",
      "283 1.22122 0.899185\n",
      "284 1.2188 0.855678\n",
      "285 1.28056 0.924226\n",
      "286 1.29001 0.0396169\n",
      "loop\n",
      "287 1.42644 0.955417\n",
      "288 1.51095 0.960495\n",
      "289 1.26383 1.051\n",
      "290 1.2006 0.935756\n",
      "291 1.31322 0.877527\n",
      "292 1.47306 0.974228\n",
      "293 1.15662 0.85384\n",
      "294 1.29022 0.754656\n",
      "295 1.18656 0.841531\n",
      "296 1.29035 0.879425\n",
      "297 1.23433 0.895437\n",
      "298 1.46212 0.97488\n",
      "299 1.42184 0.0216405\n",
      "loop\n",
      "300 1.37794 0.938813\n",
      "301 1.42322 0.888541\n",
      "302 1.19876 0.926472\n",
      "303 1.14256 0.833111\n",
      "304 1.24032 0.721709\n",
      "305 1.2945 0.780311\n",
      "306 1.17323 0.677796\n",
      "307 1.17798 0.656793\n",
      "308 1.17093 0.669737\n",
      "309 1.1606 0.708229\n",
      "310 1.20955 0.707483\n",
      "311 1.2076 0.729394\n",
      "312 1.21212 0.0222941\n",
      "loop\n",
      "313 1.44362 0.769905\n",
      "314 1.23455 0.784251\n",
      "315 1.32343 0.728351\n",
      "316 1.20555 0.84872\n",
      "317 1.45543 0.697628\n",
      "318 1.1906 0.740528\n",
      "319 1.9592 0.771773\n",
      "320 1.27585 1.05348\n",
      "321 1.426 0.761762\n",
      "322 1.32788 0.86938\n",
      "323 1.25917 0.752605\n",
      "324 1.19296 0.751793\n",
      "325 1.19612 0.00479958\n",
      "loop\n",
      "326 1.23263 0.69827\n",
      "327 1.18744 0.653477\n",
      "328 1.41873 0.707866\n",
      "329 1.71953 1.03048\n",
      "330 1.28886 0.936997\n",
      "331 1.23098 0.696118\n",
      "332 1.15707 0.528284\n",
      "333 1.19844 0.570743\n",
      "334 1.12674 0.52535\n",
      "335 1.16255 0.560608\n",
      "336 1.144 0.589198\n",
      "337 1.47821 0.628409\n",
      "338 1.48188 0.00354548\n",
      "loop\n",
      "339 1.31582 0.732474\n",
      "340 1.42204 0.647812\n",
      "341 1.14509 0.627504\n",
      "342 1.32171 0.687408\n",
      "343 1.29192 0.60342\n",
      "344 1.55029 0.821584\n",
      "345 1.18824 0.634209\n",
      "346 1.22657 0.552588\n",
      "347 1.13884 0.50601\n",
      "348 1.21372 0.504243\n",
      "349 1.14063 0.513925\n",
      "350 1.35262 0.586974\n",
      "351 1.35798 0.00216122\n",
      "loop\n",
      "352 1.17488 0.597518\n",
      "353 1.24371 0.513427\n",
      "354 1.22633 0.459547\n",
      "355 1.77326 0.63223\n",
      "356 1.69576 0.728693\n",
      "357 1.23231 0.790888\n",
      "358 1.79665 0.629341\n",
      "359 1.12631 0.65524\n",
      "360 1.34065 0.470455\n",
      "361 1.17195 0.514292\n",
      "362 1.34328 0.498697\n",
      "363 1.15754 0.550875\n",
      "364 1.15319 0.00571942\n",
      "loop\n",
      "365 1.54692 0.601058\n",
      "366 1.12732 0.51969\n",
      "367 1.43013 0.457663\n",
      "368 1.3431 0.660673\n",
      "369 1.29374 0.492892\n",
      "370 1.18743 0.445006\n",
      "371 1.28258 0.371893\n",
      "372 1.18193 0.402013\n",
      "373 1.18721 0.357276\n",
      "374 1.31275 0.402035\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-1e11e1ea2c8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mXcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_valid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_valid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mvl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_summary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrolling_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sergei/Documents/jupyter/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sergei/Documents/jupyter/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m/home/sergei/Documents/jupyter/local/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "batch_size = 128\n",
    "start_idx = 0\n",
    "idx_valid = random.sample(range(len(X)), 100)\n",
    "idx_train = np.setdiff1d(np.arange(X.shape[0]), idx_valid)\n",
    "X1 = Xcount[idx_train]\n",
    "Y1 = Y[idx_train]\n",
    "N = len(X1)\n",
    "for i in range(epochs):\n",
    "    if start_idx >= N:\n",
    "        print(\"loop\")\n",
    "        start_idx = 0\n",
    "    idx = range(start_idx, min(start_idx + batch_size, N))\n",
    "    feed_dict = {inp: X1[idx], ans: Y1[idx], keep_prob: 0.5}\n",
    "    _, l, s = c_sess.run([optimizer, loss, summary], feed_dict = feed_dict)\n",
    "    \n",
    "    writer.add_summary(s, rolling_epoch)\n",
    "    \n",
    "    feed_dict = {inp: Xcount[idx_valid], ans: Y[idx_valid], keep_prob: 1}\n",
    "    vl, vs = c_sess.run([loss, v_summary], feed_dict = feed_dict)\n",
    "    \n",
    "    writer.add_summary(vs, rolling_epoch)\n",
    "    \n",
    "    rolling_epoch += 1\n",
    "    print(rolling_epoch, vl, l)\n",
    "    start_idx += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict = {inp: XTestCount, keep_prob: 1}\n",
    "a = c_sess.run(answer, feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 5, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 1, 1, 1, 5, 0, 0, 3, 0,\n",
       "       0, 0, 1, 0, 5, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0,\n",
       "       5, 0, 3, 0, 0, 1, 0, 0, 1, 0, 3, 0, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 5,\n",
       "       0, 2, 3, 0, 2, 0, 0, 0, 0, 1, 5, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 2, 0, 5, 0, 0, 2, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 2, 3, 0, 0, 0, 3, 0, 0, 0, 1, 1, 0, 3, 0, 0,\n",
       "       0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 3, 5, 0, 0, 1, 1, 0, 0, 5, 0, 1, 0, 0, 0, 0, 2, 5, 0, 0, 0,\n",
       "       0, 0, 2, 0, 0, 1, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0,\n",
       "       1, 5, 1, 3, 1, 0, 1, 3, 0, 0, 0, 5, 1, 0, 5, 0, 0, 2, 3, 2, 0, 0, 0,\n",
       "       1, 5, 5, 0, 1, 3, 0, 0, 0, 0, 0, 5, 0, 0, 0, 3, 1, 0, 5, 1, 0, 0, 0,\n",
       "       0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 1, 0, 3, 0, 0, 0, 5, 2,\n",
       "       0, 0, 1, 1, 0, 3, 3, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 2, 0, 0, 0, 2,\n",
       "       3, 0, 0, 0, 5, 3, 0, 1, 0, 0, 0, 2, 1, 2, 0, 3, 3, 0, 0, 3, 0, 3, 3,\n",
       "       0, 5, 1, 0, 0, 0, 1, 0, 3, 0, 0, 2, 1, 0, 0, 0, 5, 3, 0, 0, 0, 0, 0,\n",
       "       5, 0, 0, 0, 5, 5, 0, 0, 0, 1, 0, 0, 2, 5, 0, 1, 5, 0, 5, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 5, 2, 0, 0, 0, 0, 0, 0, 3, 2, 5, 5, 0, 0, 0, 0, 2, 1, 2,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 1, 1, 2, 0, 2, 0])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('output.csv', 'w')\n",
    "f.write(\"id,class\\n\")\n",
    "for i in range(len(Xtest)):\n",
    "    f.write(\"{},{}\\n\".format(data_test[i]['@id'], int(a[i])))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
