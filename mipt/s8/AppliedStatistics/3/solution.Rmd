---
title: "Прикладная статистика. Задание 3: линейная регрессия"
output: html_notebook
---

Володин Сергей, группа 374

Зависимая переменная: "L10"

Имеющиеся признаки:
"Производитель" -- категориальный
"Нагрузка" -- вещественный
"Число.шаров" -- вещественный
"Диаметр" -- вещественный
"Тип.подшипника" -- категориальный

```{r}
library(gdata)
library(lmtest)
library(MASS)
data <- read.xls("bearing.xlsx")
colnames(data)
data$Производитель <- as.factor(data$Производитель)
data$Тип.подшипника <- as.factor(data$Тип.подшипника)
data_orig = data
```

```{r}
par(mfrow=c(2,2), mar=c(4, 2, 2, 1))
for(i in c(3, 4, 5)){
  d <- density(data[,i])
  plot(d, col="blue", xlab = colnames(data)[i], main="")
}
```

Категориальные признаки
```{r}
par(mfrow=c(1, 2), mar=c(4, 2, 2, 1))
for(i in c(2, 6))
  {
  hist(as.numeric(data[,i]), xlab = colnames(data)[i])
}
```

Оценка наличия выбросов
```{r}
par(mfrow=c(4,1), mar=c(4, 2, 2, 1))
for(i in c(1, 3, 4, 5)){
  boxplot(data[,i], xlab = colnames(data)[i], horizontal = TRUE)
}
```

Выбросов много. Убёрем их.
```{r}
par(mfrow=c(3,1), mar=c(4, 2, 2, 1))
out_mask = rep(FALSE, nrow(data))
for(i in c(1, 3, 4, 5))
{
  mask = data[,i] %in% boxplot(data[,i], plot = FALSE)$out
#  print(sum(mask))
  out_mask = out_mask | mask
}
#print(sum(out_mask))
data = data[!out_mask,]
```


Рассмотрим отклик: переменную L10
```{r}
boxplot(data$L10, horizontal = TRUE, xlab = colnames(data)[1])
```


Разница:
$\frac{\max L_{10}}{\min L_{10}}=$`r max(data_orig[out_mask,]$L10)/min(data_orig[out_mask,]$L10)`$>10$

Метод Бокса-Кокса
```{r}
par(mfrow=c(1,1))
boxcox(lm(L10~., data=data))
```

Берём $\lambda=\frac{1}{2}$, то есть, корень из L10

```{r}
data$newL10 <- sqrt(data$L10)
data$L10 <- NULL
```


Проведём анализ остатков. Сначала построим модель, использующую все признаки:
```{r}
m1 <- lm(data$newL10 ~., data=data)
```
```{r echo=FALSE}
shapiro.test(residuals(m1))$p.value
wilcox.test(residuals(m1))$p.value
bptest(m1)$p.value
```

Критерий     |                  |p  
----------   | -----------------|---------
Шапиро-Уилка |нормальность      |`r shapiro.test(residuals(m1))$p.value`
Стьюдента    |несмещенность     |`r t.test(residuals(m1))$p.value`
Бройша-Пагана|гомоскедастичность|`r bptest(m1)$p.value`

Остатки:

* Нормальные, поэтому используем критерий Уилкоксона, использующий только ранги.
* Несмещенные
* Гомоскедастичные

Отбор признаков
```{r}
summary(m1)
```

```{r fig.height=10, fig.width=10}
visualize_model <- function(m, data = data)
{
  par(mfrow=c(4,2))
  
  qqnorm(residuals(m))
  qqline(residuals(m), col="red")
  grid()
  
  plot(1:dim(data)[1], rstandard(m), xlab="i", ylab="Standardized residuals", pch=19)
  addtrend(1:dim(data)[1], rstandard(m))
  grid()
  
  plot(fitted(m), rstandard(m), xlab="Fitted values", ylab="Standardized residuals", pch=19)
  addtrend(fitted(m), rstandard(m))
  grid()
  
  for(i in c(1,2,3,4,5))
  {
    plot(as.numeric(data[,i]), rstandard(m), xlab=colnames(data)[i], ylab="Standardized residuals", pch=19)
    addtrend(as.numeric(data[,i]), rstandard(m))
    grid()
  }
}
visualize_model(m1)
```

Имеется квадратичная зависимость по нагрузке и диаметру.

Анализ необходимости добавления взаимодействий и квадратов признаков
```{r}
m2 <- lm(newL10 ~ . + I(Нагрузка^2) + I(Диаметр^2) + I(Диаметр^3), data=data)
summary(m2)
```

Нет изменений в характере остатков.
```{r}
shapiro.test(residuals(m2))$p.value
wilcox.test(residuals(m2))$p.value
bptest(m2)$p.value
```

Посмотрим на попарные взаимодействия:
```{r}
add1(m2, ~ .^2, test="F")
```

Нет значимых взаимодействий, удаляем незначимые признаки.

```{r}
m3 <- lm(newL10 ~ Диаметр + Производитель + Нагрузка + I(Нагрузка^2) + I(Диаметр^2) + I(Диаметр^3), data=data)
```



```{r fig.height=10, fig.width=10}
visualize_model(m3)
```


Расчёт расстояний Кука
```{r}
par(mfrow=c(1,2))
plot(fitted(m3), cooks.distance(m2), xlab="Fitted LogL10", ylab="Cook's distance")
lines(c(0,100), c(0.015, 0.015), col="red")
plot(data$newL10, cooks.distance(m3), xlab="LogL10", ylab="Cook's distance")
lines(c(0,100), c(0.015, 0.015), col="red")
```

Удалим наблюдения с расстоянием Кука больше 0.015 (порог выбран визуально) и перенастроим модель 3.
```{r, echo=FALSE}
data2 <-data[cooks.distance(m3)<=0.015,]
m4 <- lm(newL10 ~ Диаметр + Производитель + Нагрузка + I(Нагрузка^2) + I(Диаметр^2) + I(Диаметр^3), data=data2)
```

Сравним коэффициенты новой модели и модели 4:
```{r, echo=FALSE}
res <- cbind(coefficients(m3), coefficients(m4))
colnames(res) <- c("All data", "Filtered data")
res
```

Анализ модели 4
```{r fig.height=10, fig.width=10}
visualize_model(m4, data = data2)
```

Некоторые коэффициенты существенно изменились, следовательно, удаление влиятельных наблюдений имело смысл.

## Результат
Итоговая модель построена по `r dim(data2)[1]` из `r dim(data_orig)[1]` исходных объектов и объясняет `r round(100*summary(m4)$r.squared)`% вариации логарифма отклика:

При интересующих нас факторах привлекательности стоят следующие коэффициенты:
```{r, echo=FALSE}
coefficients(m4)
```