{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "Решаем задачу оптимизации $f(x)\\to\\underset{\\mathbb{R}^n}{\\min}$, где $f(x)=\\frac{1}{2}x^TAx-b^Tx$, причем матрица $A$ симметричная и положительно определенная"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотеки для работы с матрицами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание случайной матрицы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A=S^TDS$$\n",
    "S получается ортогонализацией Грамма-Шмидта из случайной матрицы, D задается явно ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Размерность пространства\n",
    "n=200\n",
    "# Случайная матрица\n",
    "S0=np.random.randn(n,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ортогонализация Грамма-Шмидта\n",
    "S=np.copy(S0)\n",
    "for i in range(n):\n",
    "    S[i] = np.copy(S0[i])\n",
    "    for j in range(i):\n",
    "        S[i] -= S[j] * np.dot(S[i], S[j]) / np.dot(S[j], S[j])\n",
    "    S[i] /= np.sqrt(np.dot(S[i], S[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверяем ортогональность\n",
    "np.allclose(np.dot(S.T,S), np.eye(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Собственные числа матрицы A\n",
    "D=np.diag(np.array(range(n)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   0   0 ...,   0   0   0]\n",
      " [  0   2   0 ...,   0   0   0]\n",
      " [  0   0   3 ...,   0   0   0]\n",
      " ..., \n",
      " [  0   0   0 ..., 198   0   0]\n",
      " [  0   0   0 ...,   0 199   0]\n",
      " [  0   0   0 ...,   0   0 200]]\n"
     ]
    }
   ],
   "source": [
    "print D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A=np.dot(S.T, np.dot(D, S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Вектор b\n",
    "b=np.random.rand(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Градиент функции\n",
    "def grad(A, b, x):\n",
    "    return(np.dot(A, x) - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция\n",
    "def func(A, b, x):\n",
    "    return(0.5 * np.dot(x.T, np.dot(A, x))-np.dot(b, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод 1. Градиентный спуск с постоянным шагом\n",
    "\n",
    "$$s_k=-\\nabla f(x_k)$$\n",
    "$$x_{k+1}=x_k+\\alpha_ks_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1148\n",
      "i_min = 1148\n",
      "func = -1.29838583644\n",
      "x = [-0.025  0.06   0.084  0.05   0.011  0.097  0.033  0.09   0.077 -0.02\n",
      "  0.046  0.031  0.011  0.026  0.064 -0.152  0.026  0.052  0.052  0.042\n",
      " -0.004  0.009  0.016 -0.016  0.04   0.001  0.018  0.05  -0.031  0.052\n",
      "  0.051 -0.005 -0.007 -0.003  0.01   0.068  0.104  0.018  0.044  0.086\n",
      " -0.019 -0.004  0.051  0.058  0.063  0.048  0.047  0.109  0.045 -0.048\n",
      " -0.048  0.057  0.011  0.033 -0.014  0.082  0.01  -0.012 -0.002  0.048\n",
      "  0.013 -0.003  0.041  0.033  0.108 -0.079 -0.045  0.     0.046  0.032\n",
      "  0.098 -0.025  0.007  0.044 -0.041  0.054 -0.03   0.021  0.136  0.002\n",
      " -0.033  0.012  0.03   0.037  0.035  0.085 -0.049  0.134 -0.049 -0.044\n",
      "  0.046  0.042 -0.041  0.01  -0.036  0.041  0.059  0.078  0.082  0.041\n",
      "  0.002 -0.004  0.013  0.035  0.078  0.065  0.181  0.007  0.066  0.028\n",
      "  0.013  0.04   0.056 -0.05  -0.107  0.011 -0.041 -0.049  0.034  0.074\n",
      "  0.015  0.019 -0.025  0.022  0.004 -0.03   0.002  0.076  0.128  0.021\n",
      " -0.041  0.003 -0.081  0.04   0.077  0.121 -0.015 -0.003  0.079  0.001\n",
      " -0.017  0.084  0.058 -0.075  0.033  0.01   0.036 -0.013 -0.039  0.01\n",
      "  0.005 -0.004  0.052  0.058 -0.006 -0.026  0.041 -0.008  0.046 -0.034\n",
      " -0.081  0.002  0.075  0.024 -0.05   0.123 -0.099  0.059  0.041 -0.005\n",
      "  0.017  0.003  0.101 -0.135 -0.066  0.065 -0.052  0.119  0.07  -0.001\n",
      " -0.073  0.051  0.055  0.026  0.073  0.048  0.013  0.005 -0.032 -0.014\n",
      "  0.03   0.043 -0.083  0.11  -0.062  0.004 -0.036  0.046 -0.085 -0.047]\n",
      "||grad|| = ||Ax-b|| = 0.000198191103624\n"
     ]
    }
   ],
   "source": [
    "x=list()\n",
    "x.append(np.zeros(n))\n",
    "i = 1\n",
    "a=0.005\n",
    "\n",
    "x_norm=1e-6\n",
    "maxiter=10000\n",
    "\n",
    "x_min = None\n",
    "f_min = None\n",
    "i_min = None\n",
    "\n",
    "while True:\n",
    "    x.append(0)\n",
    "    x[i] = x[i-1] - a * grad(A, b, x[i - 1])\n",
    "    #print x[i], func(A, b, x[i])\n",
    "    \n",
    "    f_now = func(A, b, x[i])\n",
    "    \n",
    "    if f_min == None or f_now < f_min :\n",
    "        f_min = f_now\n",
    "        x_min = x[i]\n",
    "        i_min = i\n",
    "    \n",
    "    if(linalg.norm(x[i]-x[i - 1]) < x_norm):\n",
    "        break\n",
    "    \n",
    "    if i >= maxiter:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print \"i =\", i\n",
    "print \"i_min =\", i_min\n",
    "print \"func =\", f_min\n",
    "print \"x =\", x_min\n",
    "print \"||grad|| = ||Ax-b|| =\", linalg.norm(np.dot(A, x_min) - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод 2. Градиентный спуск, выбор шага по правилу Армихо, остановка по $||\\nabla f||$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 358\n",
      "i_min = 358\n",
      "func = -1.29838585232\n",
      "x = [-0.025  0.06   0.084  0.05   0.011  0.097  0.033  0.09   0.077 -0.02\n",
      "  0.046  0.031  0.011  0.026  0.064 -0.152  0.026  0.052  0.052  0.042\n",
      " -0.004  0.009  0.016 -0.016  0.04   0.001  0.018  0.05  -0.031  0.052\n",
      "  0.051 -0.005 -0.007 -0.003  0.01   0.068  0.104  0.018  0.044  0.086\n",
      " -0.019 -0.004  0.051  0.058  0.063  0.048  0.047  0.109  0.045 -0.048\n",
      " -0.048  0.057  0.011  0.033 -0.014  0.082  0.01  -0.012 -0.002  0.048\n",
      "  0.013 -0.003  0.041  0.033  0.108 -0.079 -0.045  0.     0.046  0.032\n",
      "  0.098 -0.025  0.007  0.044 -0.041  0.054 -0.03   0.021  0.136  0.002\n",
      " -0.033  0.012  0.03   0.037  0.035  0.085 -0.049  0.134 -0.049 -0.044\n",
      "  0.046  0.042 -0.041  0.01  -0.036  0.041  0.059  0.078  0.082  0.041\n",
      "  0.002 -0.004  0.013  0.035  0.078  0.065  0.181  0.007  0.066  0.028\n",
      "  0.013  0.04   0.056 -0.05  -0.107  0.011 -0.041 -0.049  0.034  0.074\n",
      "  0.015  0.019 -0.025  0.022  0.004 -0.03   0.002  0.076  0.128  0.021\n",
      " -0.041  0.003 -0.081  0.04   0.077  0.121 -0.015 -0.003  0.079  0.001\n",
      " -0.017  0.084  0.058 -0.075  0.033  0.01   0.036 -0.013 -0.039  0.01\n",
      "  0.005 -0.004  0.052  0.058 -0.006 -0.026  0.041 -0.008  0.046 -0.034\n",
      " -0.081  0.002  0.075  0.024 -0.05   0.123 -0.099  0.059  0.041 -0.005\n",
      "  0.017  0.003  0.101 -0.135 -0.066  0.065 -0.052  0.119  0.07  -0.001\n",
      " -0.073  0.051  0.055  0.026  0.073  0.048  0.013  0.005 -0.032 -0.014\n",
      "  0.03   0.043 -0.083  0.11  -0.062  0.004 -0.036  0.046 -0.085 -0.047]\n",
      "||grad|| = ||Ax-b|| = 8.73758857065e-05\n"
     ]
    }
   ],
   "source": [
    "# Массив с точками\n",
    "x=list()\n",
    "x.append(np.zeros(n))\n",
    "i = 1\n",
    "\n",
    "# Выбор шага по Армихо, параметры\n",
    "a0 = 1\n",
    "theta = 0.6\n",
    "eps = 0.6\n",
    "\n",
    "# Результат\n",
    "x_min = None\n",
    "f_min = None\n",
    "i_min = None\n",
    "\n",
    "# Условия остановки\n",
    "grad_norm=1e-4\n",
    "maxiter=10000\n",
    "\n",
    "while True:\n",
    "    x.append(0)\n",
    "    \n",
    "    # Выбор шага по Армихо\n",
    "    a = a0\n",
    "    grad_norm_2 = np.linalg.norm(grad(A, b, x[i - 1])) ** 2\n",
    "    while True:\n",
    "        x_new = x[i - 1] - a * grad(A, b, x[i - 1])\n",
    "        if func(A, b, x_new) - func(A, b, x[i - 1]) + eps * a * grad_norm_2 < 0:\n",
    "            break\n",
    "        a *= theta\n",
    "    \n",
    "    #print a\n",
    "    \n",
    "    # Градиентный шаг\n",
    "    x[i] = x[i - 1] - a * grad(A, b, x[i - 1])\n",
    "    #print x[i], func(A, b, x[i])\n",
    "    \n",
    "    # Результат\n",
    "    x_min = x[i]\n",
    "    i_min = i\n",
    "    f_min = func(A, b, x_min)\n",
    "    \n",
    "    # Условие останова\n",
    "    if np.linalg.norm(grad(A, b, x[i])) < grad_norm:\n",
    "        break\n",
    "    \n",
    "    if i >= maxiter:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print \"i =\", i\n",
    "print \"i_min =\", i_min\n",
    "print \"func =\", f_min\n",
    "print \"x =\", x_min\n",
    "print \"||grad|| = ||Ax-b|| =\", linalg.norm(np.dot(A, x_min) - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод 3. Градиентный спуск, выбор шага по правилу одномерной минимизации, остановка по $||\\nabla f||$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 518\n",
      "i_min = 518\n",
      "func = -1.29838585245\n",
      "x = [-0.025  0.06   0.084  0.05   0.011  0.097  0.033  0.09   0.077 -0.02\n",
      "  0.046  0.031  0.011  0.026  0.064 -0.152  0.026  0.052  0.052  0.042\n",
      " -0.004  0.009  0.016 -0.016  0.04   0.001  0.018  0.05  -0.031  0.052\n",
      "  0.051 -0.005 -0.007 -0.003  0.01   0.068  0.104  0.018  0.044  0.086\n",
      " -0.019 -0.004  0.051  0.058  0.063  0.048  0.047  0.109  0.045 -0.048\n",
      " -0.048  0.057  0.011  0.033 -0.014  0.082  0.01  -0.012 -0.002  0.048\n",
      "  0.013 -0.003  0.041  0.033  0.108 -0.079 -0.045  0.     0.046  0.032\n",
      "  0.098 -0.025  0.007  0.044 -0.041  0.054 -0.03   0.021  0.136  0.002\n",
      " -0.033  0.012  0.03   0.037  0.035  0.085 -0.049  0.134 -0.049 -0.044\n",
      "  0.046  0.042 -0.041  0.01  -0.036  0.041  0.059  0.078  0.082  0.041\n",
      "  0.002 -0.004  0.013  0.035  0.078  0.065  0.181  0.007  0.066  0.028\n",
      "  0.013  0.04   0.056 -0.05  -0.107  0.011 -0.041 -0.049  0.034  0.074\n",
      "  0.015  0.019 -0.025  0.022  0.004 -0.03   0.002  0.076  0.128  0.021\n",
      " -0.041  0.003 -0.081  0.04   0.077  0.121 -0.015 -0.003  0.079  0.001\n",
      " -0.017  0.084  0.058 -0.075  0.033  0.01   0.036 -0.013 -0.039  0.01\n",
      "  0.005 -0.004  0.052  0.058 -0.006 -0.026  0.041 -0.008  0.046 -0.034\n",
      " -0.081  0.002  0.075  0.024 -0.05   0.123 -0.099  0.059  0.041 -0.005\n",
      "  0.017  0.003  0.101 -0.135 -0.066  0.065 -0.052  0.119  0.07  -0.001\n",
      " -0.073  0.051  0.055  0.026  0.073  0.048  0.013  0.005 -0.032 -0.014\n",
      "  0.03   0.043 -0.083  0.11  -0.062  0.004 -0.036  0.046 -0.085 -0.047]\n",
      "||grad|| = ||Ax-b|| = 9.84608627504e-05\n"
     ]
    }
   ],
   "source": [
    "# Массив с точками\n",
    "x=list()\n",
    "x.append(np.zeros(n))\n",
    "i = 1\n",
    "\n",
    "# С какого значения начинать поиск alpha?\n",
    "a0 = 1\n",
    "# Какая точность alpha нужна?\n",
    "a_eps = 0.01\n",
    "\n",
    "# Результат\n",
    "x_min = None\n",
    "f_min = None\n",
    "i_min = None\n",
    "\n",
    "# Условие останова\n",
    "grad_norm=1e-4\n",
    "maxiter=10000\n",
    "\n",
    "# Функция одной переменной, которую будем минимизировать\n",
    "def func1(x_, alpha):\n",
    "    return(func(A, b, x_ - alpha * grad(A, b, x_)))\n",
    "\n",
    "while True:\n",
    "    x.append(0)\n",
    "    \n",
    "    # Выбор шага: одномерная минимизация,\n",
    "    # Метод деления отрезка\n",
    "    \n",
    "    # Начальные границы отрезка\n",
    "    a_a = 0.\n",
    "    a_b = 1. * a0\n",
    "    \n",
    "    # Увеличиваем границу, пока функция не начнет увеличиваться\n",
    "    while True:\n",
    "        if func1(x[i - 1], a_b) > func1(x[i - 1], 0):\n",
    "            break\n",
    "        a_b *= 2\n",
    "    \n",
    "    #print \"amax=\", a_b\n",
    "    \n",
    "    # Ищем минимум функции одной переменной func1(x[i-1], a) по a\n",
    "    while abs(a_a - a_b) >= a_eps:\n",
    "        a_c = (a_a + a_b) / 2\n",
    "        a_y = (a_a + a_c) / 2\n",
    "        a_z = (a_y + a_b) / 2\n",
    "        \n",
    "        if func1(x[i - 1], a_y) <= func1(x[i - 1], a_c):\n",
    "            a_b = a_c\n",
    "            #a_c = a_y\n",
    "        elif func1(x[i - 1], a_c) <= func1(x[i - 1], a_z):\n",
    "            a_a = a_y\n",
    "            a_b = a_z\n",
    "        else:\n",
    "            a_a = a_c\n",
    "    \n",
    "    a = (a_a + a_b) / 2\n",
    "    #print \"a=\", a\n",
    "    \n",
    "    # Шаг\n",
    "    x[i] = x[i - 1] - a * grad(A, b, x[i - 1])\n",
    "    #print x[i], func(A, b, x[i])\n",
    "    \n",
    "    # Результат\n",
    "    x_min = x[i]\n",
    "    i_min = i\n",
    "    f_min = func(A, b, x_min)\n",
    "    \n",
    "    # Условие останова\n",
    "    if np.linalg.norm(grad(A, b, x[i])) < grad_norm:\n",
    "        break\n",
    "    \n",
    "    if i >= maxiter:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print \"i =\", i\n",
    "print \"i_min =\", i_min\n",
    "print \"func =\", f_min\n",
    "print \"x =\", x_min\n",
    "print \"||grad|| = ||Ax-b|| =\", linalg.norm(np.dot(A, x_min) - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод 4. Метод Ньютона\n",
    "Функция квадратичная $\\Rightarrow$ метод сходится за одну итерацию:\n",
    "$x_{k+1}=x_k-(\\nabla^2f(x_k))^{-1}\\nabla f(x_k)=x_k-A^{-1}(Ax-b)=A^{-1}b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_min = 1\n",
      "func = -1.29838585603\n",
      "x = [-0.025  0.06   0.084  0.05   0.011  0.097  0.033  0.09   0.077 -0.02\n",
      "  0.046  0.031  0.011  0.026  0.064 -0.152  0.026  0.052  0.052  0.042\n",
      " -0.004  0.009  0.016 -0.016  0.04   0.001  0.018  0.05  -0.031  0.052\n",
      "  0.051 -0.005 -0.007 -0.003  0.01   0.068  0.104  0.018  0.044  0.086\n",
      " -0.019 -0.004  0.051  0.058  0.063  0.048  0.047  0.109  0.045 -0.048\n",
      " -0.048  0.057  0.011  0.033 -0.014  0.082  0.01  -0.012 -0.002  0.048\n",
      "  0.013 -0.003  0.041  0.033  0.108 -0.079 -0.045  0.     0.046  0.032\n",
      "  0.098 -0.025  0.007  0.044 -0.041  0.054 -0.03   0.021  0.136  0.002\n",
      " -0.033  0.012  0.03   0.037  0.035  0.085 -0.049  0.134 -0.049 -0.044\n",
      "  0.046  0.042 -0.041  0.01  -0.036  0.041  0.059  0.078  0.082  0.041\n",
      "  0.002 -0.004  0.013  0.035  0.078  0.065  0.181  0.007  0.066  0.028\n",
      "  0.013  0.04   0.056 -0.05  -0.107  0.011 -0.041 -0.049  0.034  0.074\n",
      "  0.015  0.019 -0.025  0.022  0.004 -0.03   0.002  0.076  0.128  0.021\n",
      " -0.041  0.003 -0.081  0.04   0.077  0.121 -0.015 -0.003  0.079  0.001\n",
      " -0.017  0.084  0.058 -0.075  0.033  0.01   0.036 -0.013 -0.039  0.01\n",
      "  0.005 -0.004  0.052  0.058 -0.006 -0.026  0.041 -0.008  0.046 -0.034\n",
      " -0.081  0.002  0.075  0.024 -0.05   0.123 -0.099  0.059  0.041 -0.005\n",
      "  0.017  0.003  0.101 -0.135 -0.066  0.065 -0.052  0.119  0.07  -0.001\n",
      " -0.073  0.051  0.055  0.026  0.073  0.048  0.013  0.005 -0.032 -0.014\n",
      "  0.03   0.043 -0.083  0.11  -0.062  0.004 -0.036  0.046 -0.085 -0.047]\n",
      "||grad|| = ||Ax-b|| = 6.70582910141e-14\n"
     ]
    }
   ],
   "source": [
    "i_min = 1\n",
    "x_min = np.dot(np.linalg.inv(A),b)\n",
    "f_min = func(A, b, x_min)\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print \"i_min =\", i_min\n",
    "print \"func =\", f_min\n",
    "print \"x =\", x_min\n",
    "print \"||grad|| = ||Ax-b|| =\", linalg.norm(np.dot(A, x_min) - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод 5. Метод сопряженных градиентов\n",
    "Для СЛАУ:\n",
    "1. $x_0\\in\\mathbb{R}^n$\n",
    "2. $s_1=-\\nabla f(x_0)$\n",
    "3. $\\alpha_k=-\\frac{\\nabla^T f(x_{k-1})s_k}{s_kAs_k}$\n",
    "4. $x_k=x_{k-1}+\\alpha_ks_k$\n",
    "5. $\\beta_{k-1}=\\frac{\\nabla^T f(x_{k-1})As_{k-1}}{s_{k-1}^TAs_{k-1}}$\n",
    "6. $s_k=-\\nabla f(x_{k-1})+\\beta_{k-1}s_{k-1}$\n",
    "\n",
    "Общий случай:\n",
    "1. $x_0\\in\\mathbb{R}^n$\n",
    "2. $s_1=-\\nabla f(x_0)$\n",
    "3. $x_{k}=\\arg\\min\\limits_{\\alpha}f(x_{k-1}+\\alpha s_k)$\n",
    "4. $\\beta_{k}=(\\frac{||\\nabla f(x_k)||}{||\\nabla f(x_{k-1}||})^2$\n",
    "5. $s_{k+1}=-\\nabla f(x_{k})+\\beta_{k} s_{k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 84\n",
      "func = -1.29838585603\n",
      "x = [-0.025  0.06   0.084  0.05   0.011  0.097  0.033  0.09   0.077 -0.02\n",
      "  0.046  0.031  0.011  0.026  0.064 -0.152  0.026  0.052  0.052  0.042\n",
      " -0.004  0.009  0.016 -0.016  0.04   0.001  0.018  0.05  -0.031  0.052\n",
      "  0.051 -0.005 -0.007 -0.003  0.01   0.068  0.104  0.018  0.044  0.086\n",
      " -0.019 -0.004  0.051  0.058  0.063  0.048  0.047  0.109  0.045 -0.048\n",
      " -0.048  0.057  0.011  0.033 -0.014  0.082  0.01  -0.012 -0.002  0.048\n",
      "  0.013 -0.003  0.041  0.033  0.108 -0.079 -0.045  0.     0.046  0.032\n",
      "  0.098 -0.025  0.007  0.044 -0.041  0.054 -0.03   0.021  0.136  0.002\n",
      " -0.033  0.012  0.03   0.037  0.035  0.085 -0.049  0.134 -0.049 -0.044\n",
      "  0.046  0.042 -0.041  0.01  -0.036  0.041  0.059  0.078  0.082  0.041\n",
      "  0.002 -0.004  0.013  0.035  0.078  0.065  0.181  0.007  0.066  0.028\n",
      "  0.013  0.04   0.056 -0.05  -0.107  0.011 -0.041 -0.049  0.034  0.074\n",
      "  0.015  0.019 -0.025  0.022  0.004 -0.03   0.002  0.076  0.128  0.021\n",
      " -0.041  0.003 -0.081  0.04   0.077  0.121 -0.015 -0.003  0.079  0.001\n",
      " -0.017  0.084  0.058 -0.075  0.033  0.01   0.036 -0.013 -0.039  0.01\n",
      "  0.005 -0.004  0.052  0.058 -0.006 -0.026  0.041 -0.008  0.046 -0.034\n",
      " -0.081  0.002  0.075  0.024 -0.05   0.123 -0.099  0.059  0.041 -0.005\n",
      "  0.017  0.003  0.101 -0.135 -0.066  0.065 -0.052  0.119  0.07  -0.001\n",
      " -0.073  0.051  0.055  0.026  0.073  0.048  0.013  0.005 -0.032 -0.014\n",
      "  0.03   0.043 -0.083  0.11  -0.062  0.004 -0.036  0.046 -0.085 -0.047]\n",
      "||grad|| = ||Ax-b|| = 7.79667768411e-07\n"
     ]
    }
   ],
   "source": [
    "# Массив с точками\n",
    "x=list()\n",
    "x.append(np.ones(n) * 9)\n",
    "k = 1\n",
    "\n",
    "# Одномерная минимизация\n",
    "# Точность\n",
    "# Метод хуже работает при плохой точности одномерной минимизации\n",
    "a_eps = 1e-8\n",
    "# Можно использовать теоретическое значение для квадратичной функции\n",
    "# (строка a = ath ниже)\n",
    "\n",
    "# Начальное значение\n",
    "a0 = 1\n",
    "\n",
    "# Условие останова\n",
    "maxiter = 10000\n",
    "grad_norm = 1e-6\n",
    "\n",
    "s = -grad(A, b, x[0])\n",
    "\n",
    "# Функция одной переменной, которую будем минимизировать\n",
    "def func1(x_, alpha, s):\n",
    "    return(func(A, b, x_ + alpha * s))\n",
    "\n",
    "while True:\n",
    "    x.append(0)\n",
    "    \n",
    "    # Выбор шага: одномерная минимизация,\n",
    "    # Метод деления отрезка\n",
    "    \n",
    "    # Начальные границы отрезка\n",
    "    a_a = 0.\n",
    "    a_b = 1. * a0\n",
    "    \n",
    "    # Увеличиваем границу, пока функция не начнет увеличиваться\n",
    "    while True:\n",
    "        if func1(x[k - 1], a_b, s) > func1(x[k - 1], 0, s):\n",
    "            break\n",
    "        a_b *= 2\n",
    "    \n",
    "    #print \"amax=\", a_b\n",
    "    \n",
    "    # Ищем минимум функции одной переменной func1(x[k-1], a) по a\n",
    "    while abs(a_a - a_b) >= a_eps:\n",
    "        a_c = (a_a + a_b) / 2\n",
    "        a_y = (a_a + a_c) / 2\n",
    "        a_z = (a_y + a_b) / 2\n",
    "        \n",
    "        if func1(x[k - 1], a_y, s) <= func1(x[k - 1], a_c, s):\n",
    "            a_b = a_c\n",
    "            #a_c = a_y\n",
    "        elif func1(x[k - 1], a_c, s) <= func1(x[k - 1], a_z, s):\n",
    "            a_a = a_y\n",
    "            a_b = a_z\n",
    "        else:\n",
    "            a_a = a_c\n",
    "    \n",
    "    a = (a_a + a_b) / 2\n",
    "    #print \"a=\", a\n",
    "    #print \"s=\", s\n",
    "    #print \"x=\", x[k - 1]\n",
    "    \n",
    "    # Теоретическое значение a для квадратичных функций\n",
    "    ath = - (np.dot(grad(A, b, x[k - 1]), s)) / (np.dot(s, np.dot(A, s)))\n",
    "    #print \"ath=\", ath\n",
    "    \n",
    "    # Использовать теоретическое значение?\n",
    "    # a = ath\n",
    "    \n",
    "    # Шаг\n",
    "    x[k] = x[k - 1] + a * s\n",
    "    \n",
    "    # Результат\n",
    "    x_min = x[k]\n",
    "    f_min = func(A, b, x_min)\n",
    "    \n",
    "    # Новое s\n",
    "    grad_norm_prev = np.linalg.norm(grad(A, b, x[k - 1]))\n",
    "    grad_norm_now = np.linalg.norm(grad(A, b, x[k]))\n",
    "    \n",
    "    if grad_norm_now < grad_norm:\n",
    "        break\n",
    "    \n",
    "    beta = (grad_norm_now / grad_norm_prev) ** 2\n",
    "    #print \"beta=\", beta\n",
    "    \n",
    "    # Теоретическое значение  beta для квадратичных функций\n",
    "    betath = (np.dot(grad(A, b, x[k]), np.dot(A, s))) / (np.dot(s, np.dot(A, s)))\n",
    "    #print \"betath=\", betath\n",
    "    #beta = betath\n",
    "    \n",
    "    s = -grad(A, b, x[k]) + beta * s\n",
    "    \n",
    "    #print \"f_min=\", f_min\n",
    "    \n",
    "    if k >= maxiter:\n",
    "        break\n",
    "        \n",
    "    k += 1\n",
    "    #break\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print \"k =\", k\n",
    "print \"func =\", f_min\n",
    "print \"x =\", x_min\n",
    "print \"||grad|| = ||Ax-b|| =\", linalg.norm(np.dot(A, x_min) - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TODO:\n",
    "### Методы\n",
    "* Градиентный спуск +\n",
    "* Субградиентный спуск =град, т.к. функция дифференцируема\n",
    "* Метод Ньютона + всего 1 шаг\n",
    "* Метод сопряженных градиентов +\n",
    "\n",
    "### Критерии остановки\n",
    "не знаем $f(x^*)$ <--\n",
    "\n",
    "### Выбор шага\n",
    "По 2 методы на каждый элемент\n",
    "\n",
    "* Армихо +\n",
    "* Константа +\n",
    "* Одномерная минимизация +"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
